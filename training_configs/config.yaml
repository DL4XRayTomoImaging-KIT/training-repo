model:
checkpoint:
dataset:
optimizer:
    lr: 4e-4
    momentum: 0.9
    weight_decay: 1e-4
scheduler:
    T_max: 200
criterion:
    criterion_name: ContrastiveLoss
    criterion_hyper_parameters:
        n_negatives: 32
        n_tasks: 7

logger:
    project_name: unsupervised-segmentation
    experiment_name: sample 0.5
    log_on_batch_end: True
callbacks:
    - {'name': 'EarlyStoppingCallback', 'kwargs': {'patience': 10}}
    - {'name': 'CheckpointCallback', 'kwargs': {'save_n_best': 2}}
    - {'name': 'OptimizerCallback', 'kwargs': {'metric_key': 'loss'}}
    - {'name': 'SchedulerCallback', 'kwargs': {'reduced_metric': 'loss'}}
runner:
    runner_name: SelfSuperRunner
training:
    verbose: True
    num_epochs: 25
    logdir: /home/ws/tb0536/arina_mots/training-repo/logs/logdir/
exclude_combinations: []
attempts: null # fake variable for multirun to repeat action several times
hydra:
    run:
        # Output directory for normal runs
        dir: /mnt/data/machine-learning/arina/logs/mots/${now:%Y-%m-%d_%H-%M-%S}
    sweep:
       # Output directory for sweep runs
       dir: /mnt/data/machine-learning/arina/logs/mots/${now:%Y-%m-%d_%H-%M-%S}
       # Output sub directory for sweep runs.
       subdir: ${hydra.job.num}_${hydra.job.override_dirname}